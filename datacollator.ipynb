{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5r8AFobPOUkIPPyKMfNaj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathjams/AAAI26/blob/main/datacollator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pINKrzl8wqkz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from typing import List, Tuple\n",
        "from typing import List, Tuple, Optional\n",
        "def duplicate_indices(indices, repeats: int):\n",
        "    return [i for i in indices for _ in range(repeats)]\n",
        "\n",
        "def duplicate_indices_classwise(indices, labels, pos_times: int = 3, neg_times: int = 1):\n",
        "    out = []\n",
        "    for i in indices:\n",
        "        y = 1 if labels[i] > 0 else 0\n",
        "        r = pos_times if y == 1 else neg_times\n",
        "        out.extend([i]*r)\n",
        "    return out\n",
        "\n",
        "def _pad_slice(seq: np.ndarray, s: int, L: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Slice seq[s:s+L] with left-zero-padding if needed. Returns (win, mask).\"\"\"\n",
        "    T = len(seq)\n",
        "    if T >= s + L:\n",
        "        win = seq[s:s+L]\n",
        "        msk = np.ones((L, 2), dtype=bool)\n",
        "    else:\n",
        "        deficit = s + L - T\n",
        "        pad = np.zeros((deficit, 2), dtype=np.float32)\n",
        "        win = np.concatenate([pad, seq[s:]], axis=0)[:L]\n",
        "        m0 = np.zeros((deficit, 2), dtype=bool)\n",
        "        m1 = np.ones((L - deficit, 2), dtype=bool)\n",
        "        msk = np.concatenate([m0, m1], axis=0)\n",
        "    return win.astype(np.float32), msk\n",
        "\n",
        "def make_bag_windows_sliding(\n",
        "    seq: np.ndarray,\n",
        "    length: int,\n",
        "    hop: int,\n",
        "    *,\n",
        "    pad_short: bool = True,\n",
        "    align_last: bool = True,\n",
        "    jitter_base: bool = False,\n",
        "    seed: Optional[int] = None,\n",
        "    max_windows: Optional[int] = None,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Deterministic (or jittered) sliding windows:\n",
        "      starts = [base, base+hop, ...] up to T-length; optionally also include last_start=T-length.\n",
        "    Examples:\n",
        "      length=100, hop=50 -> [0:100], [50:150], ...\n",
        "    \"\"\"\n",
        "    assert seq.ndim == 2 and seq.shape[1] == 2, \"seq must be (T,2)\"\n",
        "    L = int(length); H = max(1, int(hop))\n",
        "    T = seq.shape[0]\n",
        "\n",
        "    rng = np.random.RandomState(seed) if seed is not None else np.random\n",
        "    starts: List[int] = []\n",
        "\n",
        "    if T >= L:\n",
        "        base = int(rng.randint(0, H)) if jitter_base else 0\n",
        "        base = max(0, min(base, H - 1))\n",
        "        starts = list(range(base, T - L + 1, H))\n",
        "        last_start = T - L\n",
        "        if align_last and (len(starts) == 0 or starts[-1] != last_start):\n",
        "            starts.append(last_start)\n",
        "    elif pad_short and T > 0:\n",
        "        starts = [0]\n",
        "    else:\n",
        "        return (torch.empty(0, L, 2), torch.empty(0, L, 2, dtype=torch.bool))\n",
        "\n",
        "    if max_windows is not None and len(starts) > max_windows:\n",
        "        idx = np.linspace(0, len(starts) - 1, num=max_windows, dtype=int)\n",
        "        starts = [starts[i] for i in idx]\n",
        "\n",
        "    wins, masks = zip(*[_pad_slice(seq, s, L) for s in starts])\n",
        "    past_values = torch.from_numpy(np.stack(wins, axis=0))        # (N,L,2)\n",
        "    past_observed_mask = torch.from_numpy(np.stack(masks, axis=0))# (N,L,2)\n",
        "    return past_values, past_observed_mask\n",
        "\n",
        "def make_bag_windows_random(\n",
        "    seq: np.ndarray,\n",
        "    length: int,\n",
        "    n_windows: int,\n",
        "    *,\n",
        "    pad_short: bool = True,\n",
        "    seed: Optional[int] = None,\n",
        "    with_replacement: bool = False,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Sample K random windows uniformly over valid start positions.\n",
        "    Good as an augmentation alternative to fixed sliding grids.\n",
        "    \"\"\"\n",
        "    assert seq.ndim == 2 and seq.shape[1] == 2, \"seq must be (T,2)\"\n",
        "    L = int(length)\n",
        "    T = seq.shape[0]\n",
        "    if T < L and not pad_short:\n",
        "        return (torch.empty(0, L, 2), torch.empty(0, L, 2, dtype=torch.bool))\n",
        "\n",
        "    rng = np.random.RandomState(seed) if seed is not None else np.random\n",
        "    if T >= L:\n",
        "        max_start = T - L\n",
        "        if with_replacement:\n",
        "            starts = [int(rng.randint(0, max_start + 1)) for _ in range(n_windows)]\n",
        "        else:\n",
        "            n = min(n_windows, max_start + 1)\n",
        "            starts = rng.choice(max_start + 1, size=n, replace=False).tolist()\n",
        "            starts.sort()\n",
        "    else:\n",
        "        starts = [0]  # will be padded\n",
        "\n",
        "    wins, masks = zip(*[_pad_slice(seq, s, L) for s in starts])\n",
        "    return torch.from_numpy(np.stack(wins, 0)), torch.from_numpy(np.stack(masks, 0))\n",
        "\n",
        "def make_bag_windows(\n",
        "    seq: np.ndarray,\n",
        "    context_length: int = 64,\n",
        "    stride: int = 16,\n",
        "    pad_short: bool = True,\n",
        "    add_noise: float = 0.0,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    seq: (T, 2) numpy float32 array\n",
        "    Returns:\n",
        "      past_values:        (Ni, L, 2) float32\n",
        "      past_observed_mask: (Ni, L, 2) bool\n",
        "    \"\"\"\n",
        "    assert seq.ndim == 2 and seq.shape[1] == 2, \"Each sequence must be (T, 2)\"\n",
        "    L = context_length\n",
        "    T = len(seq)\n",
        "    seq = seq.astype(np.float32)\n",
        "\n",
        "    starts: List[int] = []\n",
        "    if T >= L:\n",
        "        starts = list(range(0, T - L + 1, stride))\n",
        "        last_start = T - L\n",
        "        if (T - L) % stride != 0 and (len(starts) == 0 or starts[-1] != last_start):\n",
        "            starts.append(last_start)\n",
        "    elif pad_short:\n",
        "        starts = [0]\n",
        "    else:\n",
        "        return (torch.empty(0, L, 2), torch.empty(0, L, 2, dtype=torch.bool))\n",
        "\n",
        "    windows, masks = [], []\n",
        "    for s in starts:\n",
        "        if T >= s + L:\n",
        "            win = seq[s:s+L]                                  # (L, 2)\n",
        "            msk = np.ones((L, 2), dtype=bool)\n",
        "        else:\n",
        "            deficit = s + L - T\n",
        "            pad = np.zeros((deficit, 2), dtype=np.float32)\n",
        "            win = np.concatenate([pad, seq[s:]], axis=0)[:L]\n",
        "            m0 = np.zeros((deficit, 2), dtype=bool)\n",
        "            m1 = np.ones((L - deficit, 2), dtype=bool)\n",
        "            msk = np.concatenate([m0, m1], axis=0)\n",
        "\n",
        "        if add_noise > 0.0:\n",
        "            win = win + np.random.normal(0.0, add_noise, size=win.shape).astype(np.float32)\n",
        "\n",
        "        windows.append(win)\n",
        "        masks.append(msk)\n",
        "\n",
        "    past_values = torch.from_numpy(np.stack(windows, axis=0))        # (Ni, L, 2)\n",
        "    past_observed_mask = torch.from_numpy(np.stack(masks, axis=0))   # (Ni, L, 2) bool\n",
        "    return past_values, past_observed_mask\n",
        "\n",
        "def make_bag_batches(\n",
        "    bag_indices: List[int],\n",
        "    batch_size_bags: int,\n",
        "    shuffle: bool = True,\n",
        "    seed: int = 42,\n",
        ") -> List[List[int]]:\n",
        "    \"\"\"\n",
        "    Splits a list of bag indices into batches of size batch_size_bags (last batch may be smaller).\n",
        "    \"\"\"\n",
        "    idxs = list(bag_indices)\n",
        "    if shuffle:\n",
        "        rng = random.Random(seed)\n",
        "        rng.shuffle(idxs)\n",
        "    return [idxs[i:i+batch_size_bags] for i in range(0, len(idxs), batch_size_bags)]\n",
        "\n",
        "def stratified_split(indices: List[int], y: List[int], test_size: float = 0.2, seed: int = 42) -> Tuple[List[int], List[int]]:\n",
        "    rng = random.Random(seed)\n",
        "    by_cls = {}\n",
        "    for idx, lbl in zip(indices, y):\n",
        "        by_cls.setdefault(int(lbl), []).append(idx)\n",
        "    train_idx, val_idx = [], []\n",
        "    for cls, idxs in by_cls.items():\n",
        "        rng.shuffle(idxs)\n",
        "        n_val = max(1, int(round(len(idxs) * test_size))) if len(idxs) > 1 else 1\n",
        "        val_idx.extend(idxs[:n_val])\n",
        "        train_idx.extend(idxs[n_val:])\n",
        "    return train_idx, val_idx\n"
      ]
    }
  ]
}